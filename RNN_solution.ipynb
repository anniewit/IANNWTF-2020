{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_solution.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anniewit/IANNWTF-2020/blob/main/RNN_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVizrSKNFYc8"
      },
      "source": [
        "#import\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import  Model\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYmgnlvhgF8p"
      },
      "source": [
        "#Following Chris Olah here\n",
        "class LSTM_Cell(Layer):\n",
        "  def __init__(self, units):\n",
        "    super(LSTM_Cell, self).__init__()\n",
        "    self.units = units\n",
        "    self.forget_Gate = tf.keras.layers.Dense(units, activation=tf.nn.sigmoid, bias_initializer=tf.keras.initializers.Ones)\n",
        "    self.candidates = tf.keras.layers.Dense(units, activation=tf.nn.tanh, kernel_initializer='orthogonal')\n",
        "    self.candidates_gate = tf.keras.layers.Dense(units, activation=tf.nn.sigmoid, kernel_initializer='orthogonal')\n",
        "    self.out_gate = tf.keras.layers.Dense(units, activation=tf.nn.sigmoid, kernel_initializer='orthogonal')\n",
        "  \n",
        "  def call(self, x, states):\n",
        "    hidden_state, cell_state = states\n",
        "    concat_input = tf.concat((x, hidden_state), axis=-1)\n",
        "    cell_state = cell_state*self.forget_Gate(concat_input)\n",
        "    update = self.candidates(concat_input)*self.candidates_gate(concat_input)\n",
        "    cell_state = cell_state + update\n",
        "    out = tf.nn.tanh(cell_state)*self.out_gate(concat_input)\n",
        "    return out, (out, cell_state)\n",
        "\n",
        "class LSTM(Layer):\n",
        "  def __init__(self, cell):\n",
        "\n",
        "    super(LSTM, self).__init__()\n",
        "    self.cell = cell\n",
        "  \n",
        "  def call(self, x, states):\n",
        "    # x of shape[batch_size, time_steps, size]\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    # Tensor Array only needed in graph mode\n",
        "    outs = tf.TensorArray(dtype=tf.float32, size=seq_len, clear_after_read=True)\n",
        "    for t in tf.range(seq_len):\n",
        "      t_out, states = self.cell(x[:,t,:], states)\n",
        "      outs = outs.write(t, t_out)\n",
        "    out = outs.stack()\n",
        "    out = tf.transpose(out, perm=[1,0,2])\n",
        "    return out\n",
        "\n",
        "  def zero_state(self, batch_size):\n",
        "    return (tf.zeros((batch_size, self.cell.units)), tf.zeros((batch_size, self.cell.units)))\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2AgfvsnKJT9"
      },
      "source": [
        "Generate the data set for the task:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTrb1dKWKefZ"
      },
      "source": [
        "def count_task(len, num_samples):\n",
        "  for _ in range(num_samples):\n",
        "    candidate_1 = np.random.randint(0,10)\n",
        "    candidate_2 = candidate_1\n",
        "    while candidate_2 == candidate_1:\n",
        "      candidate_2 = np.random.randint(0,10)\n",
        "    count_1 = 0\n",
        "    count_2 = 0\n",
        "    inputs = []\n",
        "    for _ in range(len):\n",
        "      sample = np.random.randint(0,10)\n",
        "      inputs.append(sample)\n",
        "      if sample == candidate_1:\n",
        "        count_1 = count_1 + 1\n",
        "      if sample == candidate_2:\n",
        "        count_2 = count_2 + 1\n",
        "    input = np.asarray(inputs, dtype=np.uint8)\n",
        "    target = 1\n",
        "    if count_1 > count_2: \n",
        "      target = 0\n",
        "    context = np.asarray((candidate_1, candidate_2))\n",
        "    yield input, context, target\n",
        "\n",
        "\n",
        "SEQ_LEN = 25\n",
        "def my_count_task():\n",
        "  for elem in count_task(SEQ_LEN, 80000):\n",
        "    yield elem\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCwaWMPz51p_"
      },
      "source": [
        "class my_model(Model):\n",
        "  def __init__(self):\n",
        "    super(my_model, self).__init__()\n",
        "    self.in_layer = tf.keras.layers.Dense(64, activation=tf.nn.sigmoid)\n",
        "    self.in_layer_2 = tf.keras.layers.Dense(32)\n",
        "    self.lstm_cell = LSTM_Cell(2)\n",
        "    self.lstm = LSTM(self.lstm_cell)\n",
        "    self.out=tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
        "\n",
        "  def call(self, x):\n",
        "    batch_size = tf.shape(x)[0]\n",
        "    x = self.in_layer(x)\n",
        "    x = self.in_layer_2(x)\n",
        "    zero_state = self.lstm.zero_state(batch_size)\n",
        "    x = self.lstm(x, zero_state)\n",
        "    x = self.out(x)\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbA-8Vjms2zC"
      },
      "source": [
        "@tf.function \n",
        "def train_step(model, input, target, loss_function, optimizer, training=True):\n",
        "  target = tf.expand_dims(target, axis=-1)\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    prediction = model(input)\n",
        "    loss = loss_function(target, prediction[:,-1,:])\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  acc = tf.reduce_mean(tf.cast(tf.round(prediction[:,-1,:])==target, dtype=tf.float32))\n",
        "  return loss, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcwl0YsHrLhz"
      },
      "source": [
        "def preprocess_inputs(series, context, seq_len):\n",
        "  series = tf.one_hot(series,10)\n",
        "  context = tf.one_hot(context, 10)\n",
        "  context = tf.reshape(context, (1,20))\n",
        "  context = tf.repeat(context, seq_len, axis=0)\n",
        "  out=tf.concat((series,context), axis=-1)\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NF7nz8ybyVc",
        "outputId": "218d0ebf-ea96-4023-e740-c1e07022398b"
      },
      "source": [
        "batch_size = 256\n",
        "iters = 100\n",
        "model = my_model()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss_function = tf.losses.BinaryCrossentropy()\n",
        "\n",
        "data = tf.data.Dataset.from_generator(my_count_task, output_types=(tf.uint8, tf.uint8, tf.uint8))\n",
        "data = data.map(lambda x, y, z: (preprocess_inputs(x,y, SEQ_LEN), tf.cast(z, dtype=tf.float32)))\n",
        "data = data.shuffle(buffer_size=1000).batch(batch_size=batch_size).prefetch(20)\n",
        "\n",
        "for iter in range(iters):\n",
        "  loss_agg = []\n",
        "  acc_agg = []\n",
        "  for input, target in data:\n",
        "    loss, acc = train_step(model, input, target, loss_function, optimizer)\n",
        "    loss_agg.append(loss)\n",
        "    acc_agg.append(acc)\n",
        "  print(\"Loss %2.5f ::: Accuracy %2.5f\" % (np.mean(loss_agg), np.mean(acc_agg)))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss 0.67731 ::: Accuracy 0.58971\n",
            "Loss 0.67767 ::: Accuracy 0.58823\n",
            "Loss 0.67702 ::: Accuracy 0.59046\n",
            "Loss 0.67712 ::: Accuracy 0.59012\n",
            "Loss 0.67690 ::: Accuracy 0.59042\n",
            "Loss 0.67748 ::: Accuracy 0.58868\n",
            "Loss 0.67748 ::: Accuracy 0.58825\n",
            "Loss 0.67675 ::: Accuracy 0.58988\n",
            "Loss 0.67596 ::: Accuracy 0.58944\n",
            "Loss 0.66713 ::: Accuracy 0.60302\n",
            "Loss 0.65466 ::: Accuracy 0.61485\n",
            "Loss 0.64633 ::: Accuracy 0.62454\n",
            "Loss 0.63831 ::: Accuracy 0.63012\n",
            "Loss 0.63413 ::: Accuracy 0.63567\n",
            "Loss 0.62947 ::: Accuracy 0.64597\n",
            "Loss 0.62750 ::: Accuracy 0.64482\n",
            "Loss 0.62575 ::: Accuracy 0.64896\n",
            "Loss 0.62058 ::: Accuracy 0.65137\n",
            "Loss 0.61834 ::: Accuracy 0.65518\n",
            "Loss 0.61686 ::: Accuracy 0.65679\n",
            "Loss 0.61542 ::: Accuracy 0.65609\n",
            "Loss 0.61276 ::: Accuracy 0.65918\n",
            "Loss 0.61140 ::: Accuracy 0.66138\n",
            "Loss 0.60849 ::: Accuracy 0.66375\n",
            "Loss 0.60645 ::: Accuracy 0.66843\n",
            "Loss 0.60310 ::: Accuracy 0.66864\n",
            "Loss 0.60198 ::: Accuracy 0.67015\n",
            "Loss 0.59874 ::: Accuracy 0.67195\n",
            "Loss 0.59762 ::: Accuracy 0.67491\n",
            "Loss 0.58968 ::: Accuracy 0.68082\n",
            "Loss 0.58673 ::: Accuracy 0.68492\n",
            "Loss 0.58220 ::: Accuracy 0.68619\n",
            "Loss 0.57827 ::: Accuracy 0.69015\n",
            "Loss 0.57242 ::: Accuracy 0.69400\n",
            "Loss 0.56619 ::: Accuracy 0.69866\n",
            "Loss 0.55835 ::: Accuracy 0.70417\n",
            "Loss 0.54491 ::: Accuracy 0.71613\n",
            "Loss 0.52883 ::: Accuracy 0.72841\n",
            "Loss 0.51223 ::: Accuracy 0.74178\n",
            "Loss 0.48673 ::: Accuracy 0.76090\n",
            "Loss 0.46210 ::: Accuracy 0.77706\n",
            "Loss 0.43969 ::: Accuracy 0.78974\n",
            "Loss 0.41285 ::: Accuracy 0.80757\n",
            "Loss 0.39054 ::: Accuracy 0.82042\n",
            "Loss 0.37259 ::: Accuracy 0.83092\n",
            "Loss 0.34455 ::: Accuracy 0.84452\n",
            "Loss 0.31952 ::: Accuracy 0.85926\n",
            "Loss 0.29650 ::: Accuracy 0.87162\n",
            "Loss 0.27929 ::: Accuracy 0.87821\n",
            "Loss 0.26292 ::: Accuracy 0.88611\n",
            "Loss 0.24774 ::: Accuracy 0.89396\n",
            "Loss 0.23425 ::: Accuracy 0.90086\n",
            "Loss 0.21897 ::: Accuracy 0.90927\n",
            "Loss 0.21494 ::: Accuracy 0.90892\n",
            "Loss 0.20145 ::: Accuracy 0.91667\n",
            "Loss 0.19451 ::: Accuracy 0.91973\n",
            "Loss 0.18650 ::: Accuracy 0.92415\n",
            "Loss 0.18083 ::: Accuracy 0.92734\n",
            "Loss 0.17212 ::: Accuracy 0.93212\n",
            "Loss 0.17252 ::: Accuracy 0.92899\n",
            "Loss 0.16218 ::: Accuracy 0.93564\n",
            "Loss 0.15730 ::: Accuracy 0.93866\n",
            "Loss 0.15545 ::: Accuracy 0.93792\n",
            "Loss 0.14712 ::: Accuracy 0.94415\n",
            "Loss 0.14608 ::: Accuracy 0.94320\n",
            "Loss 0.14220 ::: Accuracy 0.94520\n",
            "Loss 0.13736 ::: Accuracy 0.94844\n",
            "Loss 0.13822 ::: Accuracy 0.94669\n",
            "Loss 0.13245 ::: Accuracy 0.95062\n",
            "Loss 0.13407 ::: Accuracy 0.94760\n",
            "Loss 0.13018 ::: Accuracy 0.94938\n",
            "Loss 0.12494 ::: Accuracy 0.95263\n",
            "Loss 0.12371 ::: Accuracy 0.95337\n",
            "Loss 0.11896 ::: Accuracy 0.95572\n",
            "Loss 0.11937 ::: Accuracy 0.95517\n",
            "Loss 0.11464 ::: Accuracy 0.95778\n",
            "Loss 0.11270 ::: Accuracy 0.95819\n",
            "Loss 0.11335 ::: Accuracy 0.95825\n",
            "Loss 0.11385 ::: Accuracy 0.95716\n",
            "Loss 0.11246 ::: Accuracy 0.95692\n",
            "Loss 0.10881 ::: Accuracy 0.96025\n",
            "Loss 0.10677 ::: Accuracy 0.96059\n",
            "Loss 0.10781 ::: Accuracy 0.95930\n",
            "Loss 0.10172 ::: Accuracy 0.96270\n",
            "Loss 0.10185 ::: Accuracy 0.96295\n",
            "Loss 0.10028 ::: Accuracy 0.96310\n",
            "Loss 0.09765 ::: Accuracy 0.96502\n",
            "Loss 0.09792 ::: Accuracy 0.96400\n",
            "Loss 0.09300 ::: Accuracy 0.96668\n",
            "Loss 0.09073 ::: Accuracy 0.96749\n",
            "Loss 0.09739 ::: Accuracy 0.96300\n",
            "Loss 0.08875 ::: Accuracy 0.96860\n",
            "Loss 0.08784 ::: Accuracy 0.96895\n",
            "Loss 0.08505 ::: Accuracy 0.97102\n",
            "Loss 0.07287 ::: Accuracy 0.97742\n",
            "Loss 0.06656 ::: Accuracy 0.98056\n",
            "Loss 0.06322 ::: Accuracy 0.98102\n",
            "Loss 0.05132 ::: Accuracy 0.98783\n",
            "Loss 0.05046 ::: Accuracy 0.98768\n",
            "Loss 0.05053 ::: Accuracy 0.98788\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}